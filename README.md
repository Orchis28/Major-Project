# Major-Project
ğŸ§‘â€ğŸ’» Temporal and Spatial Deepfake Detection using Hybrid AI (CNN + LSTM)
ğŸ“Œ Overview

Deepfakes are AI-generated synthetic videos/images that can manipulate human faces and voices, making them appear authentic. With the increasing risk of misinformation and misuse, detecting deepfakes is a critical challenge.

This project implements a hybrid AI architecture that combines Convolutional Neural Networks (CNNs) for spatial feature extraction and Long Short-Term Memory (LSTM) networks for temporal sequence modeling. Together, they enable robust detection of manipulated video content.

ğŸ¯ Key Features

Spatial Analysis (CNN): Detects frame-level artifacts such as texture inconsistencies, lighting mismatches, and blending errors.

Temporal Analysis (LSTM): Captures unnatural motion patterns, abnormal blinking, and lip-sync mismatches across video frames.

Hybrid Model: CNN encodes per-frame features â†’ LSTM models frame sequences for final classification.

Scalable Architecture: Can be extended with Transformers for improved temporal modeling.

ğŸ—ï¸ Architecture
Video Frames â†’ CNN (Feature Extraction) â†’ LSTM (Sequence Learning) â†’ Dense Layer â†’ Output (Real / Fake)


CNN â†’ Learns spatial features from each frame.

LSTM â†’ Learns temporal dependencies across frames.

Classifier â†’ Outputs probability of video being Deepfake.

ğŸ“‚ Dataset

You can experiment with public datasets such as:

FaceForensics++

DFDC (DeepFake Detection Challenge)

ğŸ¤ Contribution

Contributions are welcome! Feel free to open issues or submit pull requests.
